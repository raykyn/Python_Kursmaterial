{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling in Python\n",
    "In diesem Notebook wird in Kürze das Topic Modeling anhand der State-of-the-Union-Ansprachen der US-Präsidenten durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Textes\n",
    "with open(\"state_of_the_union.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir führen ein sehr simples Preprocessing durch, um die Daten vorzubereiten. In der Datei sind Parapgraphen durch doppelte Zeilenumbrüche getrennt und wir machen es uns einfach und setzen an diesem Punkt an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text.split(\"\\n\\n\")\n",
    "\n",
    "print(texts[56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Texte möchten wir nun noch vorverarbeiten. Dazu gehört einerseits das *tokenisieren* also auflösen in Worteinheiten, das Entfernen von Stoppwörtern und die Lemmatisierung von Wörtern, also die Reduktion auf eine Grundform (z.B. Infinitiv).\n",
    "\n",
    "Für das Preprocessing empfiehlt sich bei grösseren Projekten eine NLP-Bibliothek wie [spacy](https://spacy.io/). Spacy ist eine NLP-Bibliothek, die eine komplette Pipeline, u.a. auch Named-Entity- und POS-Tagging unterstützt.\n",
    "\n",
    "Beachte, dass wir in diesem Skript numpy für eine bestimmte Version installieren, weil Spacy uns sonst Probleme macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q numpy==1.23.5 spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# und das englische Modell\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Für Spacy müssen wir ein bestimmtes Modell herunterladen, mit dem wir die Texte vorbereiten wollen.\n",
    "# Wir verwenden das kleine englische Modell, das schnell geladen ist.\n",
    "# Wir schalten aber die Module ab, die wir nicht benötigen, damit die Texte schneller bearbeitet werden können.\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anreichern der Texte mit Informationen, das dauert eine Weile. Ich beschränke hier die Anzahl Texte auf 5000, damit das Programm schneller durchläuft.\n",
    "prepped_texts = []\n",
    "for text in texts[:5000]:\n",
    "    doc = nlp(text)\n",
    "    # Wir entfernen die Stoppwörter und wandeln die Tokens in ihre Lemmata und in Kleinbuchstaben um.\n",
    "    lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    prepped_texts.append(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Topic Modeling verwenden wir hier [Tomotopy](https://bab2min.github.io/tomotopy/v0.13.0/en/), ein Package, das in Python sehr einfach anwendbar ist.\n",
    "\n",
    "Wir upgrade ausserdem wir numpy, weil tomotopy eine neuere Version benötigt (unpraktisch, ja...). In einem realen Fall, wäre es am schlauesten, das Preprocessing und das Topic Modelling in diesem Fall zu trennen und verschiedene virtuelle Umgebungen zu verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q numpy tomotopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der nächsten Zelle trainieren wir das Modell und geben uns eine kurze Übersicht aus. Wir können beim Modell noch einige Parameter einstellen, die in der Dokumentation von Tomotopy näher erklärt werden. In unserem Fall wollen wir keine unheimlich seltenen Wörter beachten, aber auch die häufigsten entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "# Initialisiere das Modell, k gibt die Zahl der gewünschten Topics an\n",
    "mdl = tp.LDAModel(k=10, min_df=100, rm_top=10)\n",
    "\n",
    "# Lade alle Texte ins Modell\n",
    "for text in prepped_texts:\n",
    "    mdl.add_doc(text)\n",
    "\n",
    "# Trainiere das Modell, alle 50 Trainings-Iterationen wird der Stand ausgegeben\n",
    "for i in range(0, 500, 50):\n",
    "    mdl.train(100)\n",
    "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n",
    "\n",
    "# Zeige, welche Topics mit welchen wichtigen Wörtern erkannt wurden\n",
    "for k in range(mdl.k):\n",
    "    print('Top 10 words of topic #{}'.format(k))\n",
    "    print(mdl.get_topic_words(k, top_n=10))\n",
    "\n",
    "mdl.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Viewer können wir unser Modell auf einer interaktiven Oberfläche inspizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.viewer.open_viewer(mdl, port=9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir mit den Ergebnissen weiterarbeiten wollen, sollten wir sie am besten in tabellarische Form. Wir geben hier eine Tabelle aus, die Dokumente und Topics mit ihren jeweiligen Zusammenhängen zeigt. Beachte, dass hier keine Infos zu den Wort-Topic-Beziehungen gegeben sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"document_topic_matrix.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    # Schreibe den Header\n",
    "    header = [\"Document\", \"Text\"] + [f\"Topic_{i}\" for i in range(mdl.k)]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Schreibe die Daten\n",
    "    for doc_id, doc in enumerate(mdl.docs):\n",
    "        row = [f\"Doc_{doc_id}\", texts[doc_id]] + list(doc.get_topic_dist())\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(\"Document-topic matrix with text written to 'document_topic_matrix.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
