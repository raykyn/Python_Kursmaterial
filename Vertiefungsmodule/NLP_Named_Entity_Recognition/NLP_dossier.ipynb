{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxkhCkxyh8Ev"
   },
   "source": [
    "# Zusatzdossier NLP (Natural Language Processing): Arbeiten mit Regex, spacy und Flair\n",
    "\n",
    "In diesem Dossier stellen wir die Python Bibliotheken re, spacy und Flair vor. Hier lernst du die Grundlagen der beiden Bibliotheken um einfaches Textprocessing durchzuführen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYn20aSXkiw3"
   },
   "source": [
    "## Teil 1: Regex in Python\n",
    "Die [re](https://docs.python.org/3/library/re.html#) Bibliothek gibt uns die Möglichkeit, Regex in Python zu verwenden. Hier demonstrieren wir die wichtigsten Befehle der Bibliothek. Solltest du nochmals Übung mit Regex brauchen findest du auf [RegexOne](https://regexone.com/lesson/introduction_abcs) einen Überblick. Ein weiteres, sehr nützliches Tool ist [RegExr](https://regexr.com). Super zum ausprobieren und visualisieren von Regex Patterns.\n",
    "\n",
    "`re.split(pattern, string)` : Nimmt das gegebene Pattern und gibt den gespalten String in einer Liste zurück, ähnlich der string.split() Methode. Vorteil hier ist, dass ein Regex Ausdruck statt einem String gegeben werden kann, was die re.split() Methode viel leistungsfähiger macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLfCWDqBltP2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "my_string = 'Der Hund frisst eine Banane.'\n",
    "# Spaltet den String mit dem Pattern 'e(.)'\n",
    "my_list_re = re.split('e(.)', my_string)\n",
    "# Spaltet den String mit dem String 'e'. 'e(.)' ist hier nicht möglich.\n",
    "my_list_py = my_string.split('e')\n",
    "# Der Output unterscheidet sich. 'e(.)' behält den Buchstaben nach dem e und \n",
    "# fügt ihn extra in die gespaltete Liste ein.\n",
    "print(my_list_re)\n",
    "print(my_list_py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTlNJw0_v3Jp"
   },
   "source": [
    "`re.search(pattern, string)` :\n",
    "Sucht nach einem Pattern in einem String und gibt den ersten Treffer in einem Match-Objekt zurück. Mit der `match.group()` Methode kann auf den gematchten string zugegriffen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8Ukfaoou9Q5"
   },
   "outputs": [],
   "source": [
    "# Such nach dem Pattern in meinem String\n",
    "my_match = re.search('\\w*e\\w*', my_string)\n",
    "# Printe das Match Objekt\n",
    "print(my_match)\n",
    "# Printe position 0 der Match Group\n",
    "print(my_match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z23aam8zfdGA"
   },
   "source": [
    "Das `match` Objekt ist ein Datencontainer welcher mehrere Argumente wie den gematchten String und die Indices der Match Position speichert. Es kann ausserdem mehrere Gruppen matchen und ausgeben. Wenn mit Klammern im Pattern mehrere Argumente gecaptured werden, kann man auf diese mit verschiedenen Indices in `match.group()` zugreifen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zofdIAAnfdlB"
   },
   "outputs": [],
   "source": [
    "my_string = 'Donald - Duck, Ente'\n",
    "\n",
    "# Matche 'Donald Duck' und capture 'Donald' und 'Duck'\n",
    "my_match = re.search('(\\w+) - (\\w+)', my_string)\n",
    "\n",
    "# match gibt das Objekt aus\n",
    "print(my_match)\n",
    "# match.group(0) gibt das gesamte match aus.\n",
    "print(my_match.group(0))\n",
    "# match.group(1) gibt die erste capture Group\n",
    "print(my_match.group(1))\n",
    "# match.group(2) gibt die zweite capture Group\n",
    "print(my_match.group(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRUDCtlG2gkE"
   },
   "source": [
    "`re.findall(pattern, string)` : Sucht nach einem Pattern in einem String und gibt alle Treffer in einer Liste zurück.\n",
    "\n",
    "Ähnlich wie das match objekt kann `findall()` ebenfalls capture groups mmatchen und ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrNFLA6_0Plk"
   },
   "outputs": [],
   "source": [
    "my_string = 'Der Hund frisst eine Banane.\\nDie Katze rennt um den Hamster.'\n",
    "\n",
    "# Finde alle Wörter, welche ein e enthalten\n",
    "my_match = re.findall('\\w*e\\w*', my_string)\n",
    "print(my_match)\n",
    "\n",
    "# Finde alle Wörter, die mit Grossbuchstaben beginnen und nicht am anfang einer\n",
    "# Zeile stehen.\n",
    "my_match = re.findall('(\\w+) ([A-Z]\\w+)', my_string)\n",
    "print(my_match)\n",
    "\n",
    "# Finde alle Wörter, welche am Anfang einer Zeile stehen und mit D anfangen.\n",
    "my_match = re.findall('^[Dd]\\w*', my_string)\n",
    "print(my_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8xXnE7qjKWn"
   },
   "source": [
    "Wie du siehst findet das letzte `findall()` nur 'Der' am anfang des Strings. Standardmässig sieht `findall()` den \\n Character nicht als relevant für die ^ $ Syntax. Hierfür brauchen wir eine sogennante Flag. Flags können in das Pattern eingebaut werden aber das re Modul bietet hier eine etwas übersichtlichere Lösung. Statt `re.findall(pattern, string)` verwenden wir `re.findall(pattern, string, flags=re.MULTILINE)`. Sollten mehrere Flags nötig sein trennen wir diese mit |."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZbL3n-YjK2V"
   },
   "outputs": [],
   "source": [
    "# Finde alle Wörter, welche am Anfang einer Zeile stehen und mit D anfangen.\n",
    "# Berücksichtige dabei '\\n'.\n",
    "my_match = re.findall('^[Dd]\\w*', my_string, flags=re.MULTILINE)\n",
    "print(my_match)\n",
    "\n",
    "# Finde alle Wörter, welche am Anfang eines Satzes stehen und mit D anfangen.\n",
    "# Berücksichtige dabei '\\n' und ignoriere Gross-/Kleinschreibung.\n",
    "\n",
    "my_match = re.findall('^d\\w*', my_string, flags=(re.MULTILINE | re.IGNORECASE))\n",
    "print(my_match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOZcDOcz-QbH"
   },
   "source": [
    "### Beispiel\n",
    "\n",
    "Wir haben eine Textdatei mit allen züricher Orten und den zugehörigen Postleitzahlen. Wir wollen ein Dictionary anlegen, welche die Orte als Keys speichert und die Postleitzahlen als Values. Der Code dafür könnte so aussehen:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDsP42h1_FXT"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# unser Dictionary\n",
    "plz_zürich = defaultdict(list)\n",
    "# öffnet unser Textfile als 'infile' und liest die daten\n",
    "with open('Postleitzahlen.txt', 'r', encoding='utf8') as infile:\n",
    "    raw_data = infile.read()\n",
    "\n",
    "# Regex Pattern das ein Tupel aus (plz, ort) aus jeder Zeile liest\n",
    "plz_place = re.findall('^(\\d*)\\t([\\w| ]*)\\t', raw_data, flags=re.MULTILINE)\n",
    "\n",
    "for pair in plz_place:\n",
    "    # dank defaultdict müssen wir nicht prüfen, ob der Key schon im Dict ist\n",
    "    plz_zürich[pair[1]].append(pair[0])\n",
    "\n",
    "# Geben wir uns das Dict alphabetisch sortiert aus:\n",
    "for name, plz in sorted(plz_zürich.items()):\n",
    "    print(f'{name:{21}}: {plz}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rpcbq3gptw8r"
   },
   "source": [
    "Hier das [Real Python](https://realpython.com/sort-python-dictionary/#using-the-sorted-function) Tutorial für `sorted()` (eventuall muss hochgescrollt werden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zg8B2i_Xw9BN"
   },
   "source": [
    "Ein fortgeschrittenes Modul für das Arbeiten mit Regex ist das Modul `regex`. Es bieted alles was `re` kann, hat aber einige erweiterte Funktionen wie das Suchen nach meherern Patterns gleichzeitig und das erkennen von überlappenden Matches. Es ist komplexer als `re` und sichere Kenntnisse in Python und Regex sind von Vorteil. Hier die [Dokumentation](https://pypi.org/project/regex/).\n",
    "\n",
    "`regex` ist nicht Teil der Python Standardbibliothek und muss deshalb erst installiert werden. \n",
    "Für die installation den Commmand\n",
    "\n",
    "`%pip install regex`\n",
    "\n",
    "verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udcKt9MCbfmG"
   },
   "source": [
    "## Teil 2: Textbearbeitung in Python\n",
    "Um Textdaten auszuwerten ist es wichtig zu wissen, wie man Textdateien einliest und in auswertbare Form bringt. Hier lernst du den Standardprozess und Best-Practises um schnell mit Natural Language zu arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNQaa8GE2W1u"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# einlesen des Textdokumentes:\n",
    "with open('buddenbrooks.txt', 'r', encoding='utf8') as infile:\n",
    "  raw_text = infile.read()\n",
    "  print(raw_text[2500:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtj6XTv63vrd"
   },
   "outputs": [],
   "source": [
    "# Spalten des Textes in Wörter\n",
    "# (eine sehr primitive Methode in diesem Fall, besser man verwendet dazu eine Bibliothek wie spacy oder NLTK)\n",
    "words = re.split('[\\n \\.,;:!\\?»«©\\']', raw_text[2500:3000])\n",
    "print(words)\n",
    "\n",
    "# Lowercasing\n",
    "for i in range(len(words)-1):\n",
    "  words[i] = words[i].lower()\n",
    "print(words)\n",
    "\n",
    "# Herausfiltern der Leer-Wörter\n",
    "words = [word for word in words if word != '']\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gggb5ZhO_y9W"
   },
   "source": [
    "### Beispiel\n",
    "Den vorverarbeiteten Text können wir z.B. mithilfe eines Counters sehr einfach auszählen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke68wZ-p5MH2"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Zählen der Wörter\n",
    "word_count = Counter(words)\n",
    "\n",
    "# Ausgabe der ersten 10 worte:\n",
    "for word, count in word_count.most_common(10):\n",
    "    print(f'{word:{16}}{count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfQZmn0flQIv"
   },
   "source": [
    "## Teil 3: Textbearbeitung mit spaCy\n",
    "\n",
    "In diesem Teil behandeln wir [spaCy](https://spacy.io/usage/spacy-101), eine Bibliothek welche viele nützliche Funktionen für Textbearbeitung und Analyse bringt. Hier schauen wir uns die wichtigsten mit einigen Beispielen an."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufsLube_aCmt"
   },
   "source": [
    "### Machine Learning für Text Processing\n",
    "\n",
    "[Machine Learning](https://www.lexalytics.com/blog/machine-learning-natural-language-processing/#:~:text=Machine%20learning%20for%20NLP%20and,known%20as%20supervised%20machine%20learning.) beschreibt die Fähigkeit von Computern, menschliches Verhalten zu imitieren. Im bereich Natural Language Processing bedeutet das unter anderem, aus vorannotierten Texten zu lernen und das gelernte auf 'raw data' anzuwenden. Also Texte korrekt in Sätze und Tokens zu spalten und Wortarten und Lemmata erkennen und selber annotieren zu können.\n",
    "\n",
    "SpaCy bietet ein solches Machine Learning Modell. Es ist in der Lage, Texte in Tokens zu spaltern und mit recht hoher Genauigkeit diese mit allen möglichen Attributen zu annotieren ([Dokumentation](https://spacy.io/api/doc)). Spacy ist nur eine unter vielen NLP und Machine-Learning-Bibliotheken. Sie ist insbesondere auf auch für Anfänger leicht zu verwenden und läuft auch auf Rechnern ohne Deep-Learning-fähige Grafikkarte, dafür liefert sie nicht unbedingt die besten Resultate. \n",
    "\n",
    "Weitere nennenswerte Bibliotheken sind **SciKit-Learn** (Super Dokumentation inkl. Artikeln zur Theorie hinter den Funktionen, anfängerfreundlich, super für auch für unsupervisiertes Lernen), **FlairNLP** (Komplexer, aber State-of-the-Art Sequence Tagging), **NLTK** (etwas veraltet, aber sehr ausführliche Dokumentation, anfängerfreundlich) und **Transformers** (Top-aktuelles Deep-Learning, sehr komplex).\n",
    "\n",
    "SpaCy ist keine Python-Integrierte Bibliothek. Wir müssen sie also zuerst installieren. Ausserdem arbeitet spaCy mit Language Models. Diese Dateien sind sehr gross und es gibt recht viele, weshalb spaCy sie nicht automatisch lädt. Für diesen Teil laden wir das de_core_news_small Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z47Y18NHNsTA"
   },
   "outputs": [],
   "source": [
    "# Download und Upgrade von spaCy\n",
    "%pip install -U -q spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacys trainierte Modelle werden in sogenannten Modulen bereitgestellt. Wenn wir also z.B. einen deutschsprachigen Text verarbeiten möchten, müssen wir ein entsprechendes Modul herunterladen. In diesem Beispiel laden wir ein Modell herunter, das auf deutschen Zeitschriftentexten trainiert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2fSXMdMN6SM"
   },
   "outputs": [],
   "source": [
    "# Download des spaCy Sprachenmoduls de-core-news-small\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_CsxP_pFiHO"
   },
   "source": [
    "In Spacy sind alle Schritte in einer sogenannte Pipeline gesammelt. Wenn wir einen String in die Pipeline geben, erstellt Spacy ein Document-Objekt, welches in jedem Schritt weitere Informationen erhält.\n",
    "\n",
    "![Pipeline](pipeline.png)\n",
    "\n",
    "In der Dokumentation des [Modells](https://spacy.io/models/de) können wir alle Komponenten der Pipeline des jeweiligen Modells sehen. Bei de-core-news-small haben wir also:\n",
    "- Einen Tokenizer (nicht extra angegeben), wird immer laufen gelassen\n",
    "- tok2vec : Mithilfe eines Sprachmodells werden die Token in Vektoren umgewandelt, ein grundlegender Schritt für die meisten NLP-Anwendungen\n",
    "- tagger : Ermittelt zu jedem Token einen Part-Of-Speech-Tag (z.B. ob es ein Verb, Nomen oder Adjektiv ist)\n",
    "- morphologizer : Ermittelt zu jedem Token die Morphologie\n",
    "- parser : Lässt einen Dependency-Parser über den String laufen\n",
    "- lemmatizer : Ermittelt zu jedem Token das Lemma\n",
    "- attribute_ruler : Enthält je nach Modell Regeln um Token weiter zu klassifizieren\n",
    "- ner : Klassifiziert, bei welchen Token es sich um *Named Entities* handelt\n",
    "\n",
    "Manche dieser Komponenten sind sehr rechenintensiv. Wenn wir nur an bestimmten Komponenten interessiert sind, können wir sie abschalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3P67fmEnWwl"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Lädt die Pipeline als 'nlp'\n",
    "nlp = spacy.load('de_core_news_sm')\n",
    "\n",
    "# Öffnet das Textfile als 'infile'\n",
    "with open('buddenbrooks.txt', 'r', encoding='utf8') as infile:\n",
    "    # Ohne Split ist der Text zu lang. Eine Möglichkeit wäre z.B. an Kapitel zu splitten.\n",
    "    # Hier splitten wir einfach nach 100000 Zeichen.\n",
    "    raw_text = infile.read(100000)\n",
    "    # Wir überspringen ausserdem die ersten 2000 Zeichen, wo nur die Metadaten stehen.\n",
    "    raw_text = raw_text[2000:]\n",
    "    # Verarbeitet den Text mit der Pipeline und speichert das Ergebnis in 'doc'\n",
    "    doc = nlp(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq3wq1GdIZ7m"
   },
   "source": [
    "In der Variable `doc` haben wir nun ein Document-Objekt. Dieses können wir z.B. iterieren, um uns die Token anzusehen. Die Token enthalten die Informationen, welche wir in der Pipeline hinzugefügt haben als Attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymx-AYxmIaO4"
   },
   "outputs": [],
   "source": [
    "# Nur ein Ausschnitt des Textes\n",
    "for token in doc[100:500]:   \n",
    "    # überspringen von Whitespaces\n",
    "    if token.pos_ == \"SPACE\":\n",
    "        continue\n",
    "    # wir geben den String des Tokens, den POS-Tag, das Lemma und ob es eine NE ist aus\n",
    "    print(f'{token.text:{16}}{token.pos_:{12}}{token.lemma_:{12}}{token.ent_iob_+\"-\"+token.ent_type_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Dokumenten-Objekt ermöglicht aber auch Analysen auf der Dokumenten-Ebene\n",
    "# Hier z.B. lassen wir uns alle Named Entities im Dokument anzeigen\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:{24}}{ent.label_}\")\n",
    "\n",
    "# Schau dir die Resultate an, wie ist die Qualität?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2w-7DSIcJd_I"
   },
   "source": [
    "### Beispiel\n",
    "\n",
    "Nun wollen wir wie in Teil 2 die häufigsten Wörter zählen. Allerdings werden wir hier die Lemmata und nicht die Wortformen benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDg9QQR1KUZy"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "lemmata = [token.lemma_ for token in doc if token.pos_ != \"SPACE\"]\n",
    "\n",
    "counter = Counter(lemmata)\n",
    "\n",
    "for word, count in counter.most_common(10):\n",
    "    print(f'{word:{16}}{count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teil 4: Sequence-Tagging mit FlairNLP\n",
    "In diesem Kapitel stelle ich eine weitere Bibliothek vor, die weitaus einfacher als die Transformer-Bibliothek verwendbar ist, und trotzdem State-of-the-Art Resultate produziert. Das ist die [FlairNLP](https://flairnlp.github.io/)-Bibliothek.\n",
    "\n",
    "Ein Hinweis zu diesem Kapitel: Hierfür muss flair installiert werden, eine Bibliothek die umfangreicher ist als z.B. spacy. Denke darin in environments zu arbeiten, so dass du im Zweifelsfall die Bibliothek leicht deinstallieren kannst, wenn du sie nicht benötigst.\n",
    "\n",
    "Dieses Tutorial konzentriert sich auf die Anwendung, in welcher FlairNLP die besten Leistungen zeigt, nämlich die Annotation von *Named Entities*, eine Aufgabe die als *Sequence Tagging* modelliert wird. *Sequence Tagging* bedeutet eine Sequenz (von Token) rein, eine Sequenz (von Tags) raus. FlairNLP bietet inzwischen aber auch Funktionen zur Dokumentenklassifikation und Sentimentanalyse.\n",
    "\n",
    "Etwas Hintergrund zu FlairNLP: Der Grund, wieso die Architektur solch guten Leistungen zeigt, ist in ihren speziellen Sprachmodellen zu finden. Diese vektorisieren Token nicht pro Token, sondern berechnen das Encoding jedes Tokens auf Basis seiner Zeichen und des Kontexts. Auf der Webseite von FlairNLP sind auch Tutorials zu finden, wie man seine eigenen Modelle trainieren kann. Dies ist in FlairNLP meiner Meinung nach einfacher als in Spacy. \n",
    "FlairNLP basiert aber auf einer *Deep Learning*-Architektur und ist dadurch rechnerisch anspruchsvoller. Will man ein Modell trainieren oder grössere Mengen an Text verarbeiten, empfiehlt sich eine Infrastruktur mit entsprechend geeigneter Grafikkarte. Für kleinere Experimente kann aber schon [Google Colab](https://colab.research.google.com/) verwendet werden (Nutzung von TPUs/GPUs muss in Notebook Settings aktiviert werden!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlairNLP installieren\n",
    "%pip install -U flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Den Text einlesen\n",
    "with open('buddenbrooks.txt', 'r', encoding='utf8') as infile:\n",
    "    raw_text = infile.read(100000)  # Wir nehmen auch hier nur die ersten 100000 Zeichen\n",
    "    raw_text = raw_text[2000:]  # und überspringen die ersten 2000 Zeichen um die Metadaten zu entfernen\n",
    "\n",
    "# FlairNLP beinhaltet kein Satz-Splitting, wir machen das ganz primitiv an Punkten\n",
    "# Für eine richtige Pipeline könnten wir z.B. den Splitter von spacy benutzen\n",
    "sentences = re.split(r\"[.\\?!;]\", raw_text)\n",
    "\n",
    "# Satz-Splitting ist wichtig weil sich die Performance des Flair-Taggers mit längerwerdenden Sequenzen verschlechtert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispiel verwenden wir eines der vortrainierten Modelle von FlairNLP. Weitere Modelle finden sich auf [Huggingface](https://huggingface.co/models?library=flair) mit der Filteroption *Flair*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir können mit Flair Modelle von Huggingface.co verwenden\n",
    "# oder welche die wir lokal installiert haben (zb selbsttrainiert)\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# Hier laden wir das Modell um deutschsprachige Entitäten zu erkennen\n",
    "# Vorsicht, das Modell ist relativ gross (>1.4GB)\n",
    "# Wenn es einmal heruntergeladen wurde, bleibt es im Cache gespeichert für eine Weile, muss also nicht jedes Mal neu heruntergeladen werden\n",
    "tagger = SequenceTagger.load(\"flair/ner-german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# zur Verarbeitung wandeln wir noch unsere Strings in Sentence-Objekte um\n",
    "# sie erfüllen bei flair eine ähnliche Funktion wie Document-Objekte in spacy\n",
    "flair_sentences = [Sentence(sentence) for sentence in sentences if sentence != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wir können eine Liste von Sätzen auf einmal taggen lassen (dauert eventuell ein paar Minuten, im Zweifelsfall nur auf den ersten paar Sätzen testen)\n",
    "tagger.predict(flair_sentences)\n",
    "\n",
    "# Kein Return-Objekt, die Informationen werden direkt im jeweiligen Sentence-Objekt gespeichert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sehen wir uns die Resultate an\n",
    "for sentence in flair_sentences[:10]:\n",
    "    # Flair druckt jeweils den Satz aus, gefolgt von den erkannten Entitäten\n",
    "    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wollen wir nur die Entitäten extrahieren:\n",
    "for sentence in flair_sentences:\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        print(entity)\n",
    "\n",
    "# Was ist dein Eindruck im Vergleich zur Performance von spacy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell selber trainieren\n",
    "In diesem Tutorial werden wir kein eigenes Modell trainieren. Das [Flair Tutorial](https://flairnlp.github.io/docs/category/tutorial-2-training-models) enthält aber übersichtliche Anleitungen, wie man selbst ein Modell von Grund auf oder fine-tunen kann."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
