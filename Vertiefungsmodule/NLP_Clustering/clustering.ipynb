{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering mit Scikit-Learn\n",
    "In diesem Tutorial werden wir uns mit dem Thema Clustering beschäftigen. Clustering ist eine unüberwachte Lernmethode, die es ermöglicht, Daten in Gruppen zu unterteilen, ohne dass vorherige Labels oder Kategorien bekannt sind. Wir werden die Bibliothek Scikit-Learn verwenden, um verschiedene Clustering-Algorithmen zu implementieren und zu visualisieren.\n",
    "\n",
    "In diesem Beispiel werden wir mit der Sammlung von State-of-the-Union-Ansprachen arbeiten. Beachte, dass diese uns erlaubt, auf komplexeres Preprocessing zu verzichten, weil sie auf englisch ist. Für einen deutschen Text ist ein Vorgehen wie im Tutorial zu Topic Modeling empfehlenswert um z.B. Wortformen zu normalisieren. Auch ein englischer Text könnte aber von einem ausführlicheren Preprocessing profitieren. In diesem Tutorial wurde zu Gunsten der Kürze darauf verzichtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen des Textes\n",
    "with open(\"state_of_the_union.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir teilen den Text in Absätze auf, sehr rudimentär\n",
    "texts = text.split(\"\\n\\n\")\n",
    "\n",
    "print(texts[56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Bibliothek [scikit-learn](https://scikit-learn.org/stable/) ist eine der am häufigsten verwendeten Bibliotheken für maschinelles Lernen in Python. Sie bietet eine Vielzahl von Algorithmen und Tools für verschiedene Aufgaben, einschließlich Klassifikation, Regression und Clustering. In diesem Tutorial konzentrieren wir uns auf die Clustering-Algorithmen in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q scikit-learn numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Texte zu clustern, müssen wir sie in Vektoren (Embeddings) umwandeln. Wir könnten dies auf verschiedene Arten tun, z.B. auch mithilfe eines Sprachmodells aus einem Framework wie Flair oder Spacy. Wir behelfen uns hier mit einer einfacheren Methode [tf-idf](https://de.wikipedia.org/wiki/Tf-idf), welche Texte durch relative Worthäufigkeit repräsentiert. Diese Methode ist einfach zu implementieren und liefert in vielen Fällen gute Ergebnisse. Wir verwenden die Klasse `TfidfVectorizer` aus Scikit-Learn, um die Texte in Vektoren umzuwandeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vorbereiten der Vektorisierung, Stoppwörter werden ebenso wie besonders häufige Wörter entfernt.\n",
    "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, max_df=0.8, min_df=2)\n",
    "\n",
    "# Berechnen der Vektoren\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# In einem TF-IDF-Embedding hat jeder Absatz einen Vektor, und jede Position im Vektor steht für ein Wort im Vokabular des ganzen Korpus.\n",
    "# Der Wert für das Wort ist grösser, wenn das Wort in diesem Absatz relativ häufig erscheint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispiel-Notebook verwenden wir KMeans als Clustering-Algorithmus. Es ist ein schneller Algorithmus, bei dem man die Zahl der gewünschten Cluster im Voraus angibt. Scikit-Learn erklärt die Funktionsweise des Algorithmus in ihrem [User Guide](https://scikit-learn.org/stable/modules/clustering.html#k-means). Eine tolle interaktive Erklärung findet sich auch [hier](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/).\n",
    "\n",
    "KMeans ist nur ein möglicher Clustering-Algorithmus. Je nach Daten und gewünschten Clustern könnte ein anderer Algorithmus sich besser eignen. Scikit-Learn bietet in seinem User Guide eine gute Übersicht über die üblichen Clustering-Algorithmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Gewünschte Zahl der Cluster\n",
    "k = 10\n",
    "# Vorbereiten des KMeans-Algorithmus\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# Berechnen der Cluster\n",
    "cluster_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Cluster Labels ist eine Liste mit der gleichen Länge wie die Anzahl der Absätze, auf jeder Position befindet sich eine Zahl für das zugewiesene Cluster.\n",
    "print(cluster_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den folgenden zwei Zellen untersuchen wir unsere Cluster. In der ersten Zelle, mit recht komplexem Code, untersuchen wir, welche Wörter pro Cluster besonders relevant sind, um ähnlich wie beim Topic Modeling einen Eindruck über den Inhalt des Clusters zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy ist eine Bibliothek, die für komplexe mathematische Operationen grundlegend ist.\n",
    "import numpy as np\n",
    "\n",
    "# Wie viele Wörter sollen ausgegeben werden?\n",
    "top_n = 10\n",
    "# Anzahl Cluster\n",
    "num_clusters = k\n",
    "# Welches Wort befindet sich auf welcher Position in der Tf-idf-Matrix?\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "top_words_per_cluster = []\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    # Sammle alle Vektoren der Absätze, die in diesem Cluster sind\n",
    "    cluster_indices = np.where(cluster_labels == i)[0]\n",
    "    if len(cluster_indices) == 0:\n",
    "        top_words_per_cluster.append([])\n",
    "        continue\n",
    "\n",
    "    # Berechne den Durchschnitt der TF-IDF-Werte für alle Absätze in diesem Cluster\n",
    "    cluster_tfidf = tfidf_matrix[cluster_indices]\n",
    "    cluster_mean = cluster_tfidf.mean(axis=0)  # mean over all documents in cluster\n",
    "\n",
    "    # Extrahiere die Top-N Wörter\n",
    "    cluster_mean_array = np.asarray(cluster_mean).flatten()\n",
    "    top_indices = np.argsort(cluster_mean_array)[::-1][:top_n]\n",
    "    top_words = [feature_names[idx] for idx in top_indices]\n",
    "    \n",
    "    top_words_per_cluster.append(top_words)\n",
    "\n",
    "# Die Top-Wörter ausgeben\n",
    "for i, words in enumerate(top_words_per_cluster):\n",
    "    print(f\"\\n--- Cluster {i} ---\")\n",
    "    print(\", \".join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Zufällige Absätze aus jedem Cluster auswählen\n",
    "for i in range(num_clusters):\n",
    "    # Alle Absätze in diesem Cluster\n",
    "    cluster_indices = np.where(cluster_labels == i)[0]\n",
    "    if len(cluster_indices) > 0:\n",
    "        # Suche zufällig einen Absatz aus\n",
    "        random_index = random.choice(cluster_indices)\n",
    "        print(f\"\\n--- Cluster {i} ---\")\n",
    "        print(texts[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und was wäre bloss ein Clustering ohne Visualisierung? In unserem Fall eignet sich besonders eine Visualisierung über Zeit, da aber die Metadaten zu den Texten nicht vorhanden sind, nutzen wir einfach den Fakt, dass unsere Liste zeitlich sortiert ist (frühere Ansprachen zuerst). Im ersten Beispiel ist die Vorstellung mit der Bibliothek matplotlib realisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U -q matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wir verwenden pandas um die Daten leichter zu organisieren\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "\n",
    "# Um die Cluster im Zeitverlauf zu analysieren, verwenden wir einen gleitenden Zeitfensteransatz.\n",
    "window_size = 20  # so viele Absätze in einem Fenster\n",
    "step = 5  # so weit bewegt sich das Fenster in jedem Schritt\n",
    "\n",
    "time_bins = []\n",
    "cluster_counts = []\n",
    "\n",
    "# Berechnung der Clusterverteilung in jedem Zeitfenster\n",
    "for start in range(0, len(df) - window_size + 1, step):\n",
    "    end = start + window_size\n",
    "    window = df.iloc[start:end]\n",
    "    counts = window['cluster'].value_counts(normalize=True).sort_index()\n",
    "    cluster_counts.append(counts)\n",
    "    time_bins.append(start + window_size // 2)  # midpoint of window\n",
    "\n",
    "# Leere Fenster mit Null-Werten füllen\n",
    "cluster_matrix = pd.DataFrame(cluster_counts).fillna(0)\n",
    "\n",
    "# Den Graph erstellen\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.stackplot(time_bins, cluster_matrix.T.values, labels=[f\"Cluster {i}\" for i in cluster_matrix.columns])\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1.0))\n",
    "plt.title(\"Cluster Trends Over Time\")\n",
    "plt.xlabel(\"Time (sliding window index)\")\n",
    "plt.ylabel(\"Proportion of Texts\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Hier sehen wir schon gewisse Farbverläufe, die auf eine gewisse Entwicklung der Themen hinweisen. Aber das ganze ist noch etwas unübersichtlich und seltene Cluster sind kaum erkennbar. Um eine interaktive Untersuchung des ganzen zu bewerkstelligen, kann uns die Bibliothek [plotly](https://plotly.com/python/) helfen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Wir bereiten die Daten wie zuvor auf\n",
    "df = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'cluster': cluster_labels\n",
    "})\n",
    "\n",
    "window_size = 20\n",
    "step = 5\n",
    "time_bins = []\n",
    "cluster_counts = []\n",
    "\n",
    "for start in range(0, len(df) - window_size + 1, step):\n",
    "    end = start + window_size\n",
    "    window = df.iloc[start:end]\n",
    "    counts = window['cluster'].value_counts(normalize=True).sort_index()\n",
    "    cluster_counts.append(counts)\n",
    "    time_bins.append(start + window_size // 2)\n",
    "\n",
    "cluster_matrix = pd.DataFrame(cluster_counts).fillna(0)\n",
    "\n",
    "# Dieses Mal erstellen wir also einen interaktiven Graphen\n",
    "fig = go.Figure()\n",
    "\n",
    "for cluster in cluster_matrix.columns:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_bins,\n",
    "        y=cluster_matrix[cluster],\n",
    "        mode='lines',\n",
    "        name=f'Cluster {cluster}',\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cluster Trends Over Time (Interactive)',\n",
    "    xaxis_title='Text Index (Sliding Window Midpoint)',\n",
    "    yaxis_title='Proportion of Cluster',\n",
    "    legend_title='Clusters',\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white',\n",
    "    width=1000,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das war es erstmal für diese Einführung! Beachte, dass wir diese Auswertung noch an zahlreichen Stellen verbessern könnten:\n",
    "- Besseres Preprocessing (insbesondere Lemmatisierung)\n",
    "- Ergänzen der Texte mit Metadaten z.b. zeitlicher Information\n",
    "- Maskieren von Named Entities, damit sie im Tf-idf die Vektoren weniger verzerren\n",
    "- Verwendung von anderen Clustering-Algorithmen\n",
    "- Verwendung von anderen Vektorisierungs-Methoden (z.B. Word2Vec, BERT, etc.)\n",
    "\n",
    "Viel Spass beim Experimentieren!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
